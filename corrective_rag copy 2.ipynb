{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ollama pull  mistral:instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm=\"mistral:instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_mistralai  import MistralAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Création index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération données (page web) servant de correction au RAG\n",
    "url=\"https://france3-regions.francetvinfo.fr/auvergne-rhone-alpes/puy-de-dome/clermont-ferrand/elections-legislatives-2024-majorite-absolue-et-relative-cohabitation-motion-de-censure-le-vocabualire-pour-bien-comprendre-le-fonctionnement-de-l-assemblee-nationale-2993165.html\"\n",
    "loader=WebBaseLoader(url)\n",
    "docs=loader.load()\n",
    "\n",
    "#découpage données\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=100\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#representation vectorielle\n",
    "embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#base de données vectorielles\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"rag_chroma\",\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "retriever=vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Vous êtes un évaluateur évaluant la pertinence d'un document récupéré par rapport à une question d'un utilisateur. \\n \n",
    "    Voici le document récupéré : \\n\\n {document} \\n\\n\n",
    "    Voici la question de l'utilisateur : {question} \\n\n",
    "    Si le document contient des mots-clés liés à la question de l'utilisateur, notez-le comme pertinent. \\n\n",
    "    Il n est pas nécessaire que ce soit un test rigoureux. Le but est de filtrer les récupérations erronées. \\n\n",
    "    Donnez un score binaire 'yes' ou 'no' pour indiquer si le document est pertinent par rapport à la question. \\n\n",
    "    Fournissez le score binaire au format JSON avec un « score » à clé unique et sans prémisse ni explication.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"Comment fonctionne une cohabitation ?\"\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "doc_txt = docs[1].page_content\n",
    "score = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph State\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,TypedDict,Dict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.schema import Document\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Représente l'état de Nontre graphique\n",
    "    \n",
    "    Attributes: \n",
    "        keys: Un dictionnaire où chaque clé est une chaîne.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str,any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain import hub\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Récupère les documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): L'état actuel du graphique\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Nouvelle clé ajoutée à l'état, documents, qui contient les documents récupérés\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict=state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "\n",
    "    # Récupération\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    return {\"keys\":{\"documents\": documents, \"question\": question}}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Génère la réponse \n",
    "\n",
    "    Args:\n",
    "        state (dict): L'état actuel du graphique\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Nouvelle clé ajoutée à l'état, génération, qui contient la génération\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict= state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    prompt=hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    llm= ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \n",
    "                 \"question\": question, \n",
    "                 \"generation\": generation}\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Détermine si les documents récupérés sont pertinents pour la question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): L'état actuel du graphique\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Met à jour la clé des documents avec uniquement les documents pertinents filtrés\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    state_dict= state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    llm= ChatOllama(model=local_llm,format=\"json\",temperature=0)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Vous êtes un évaluateur évaluant la pertinence d'un document récupéré par rapport à une question d'un utilisateur. \\n\n",
    "            Voici le document récupéré: \\n\\n {document} \\n\\n\n",
    "            Voici la question de l'utilisateur: {question} \\n\n",
    "            Si le document contient des mots-clés liés à la question de l'utilisateur, notez-le comme pertinent. \\n\n",
    "            Il n est pas nécessaire que ce soit un test rigoureux. Le but est de filtrer les récupérations erronées. \\n\n",
    "            Donnez un score binaire 'Yes' ou 'No' pour indiquer si le document est pertinent par rapport à la question. \\n\n",
    "            Fournissez le score binaire au format JSON avec un « score » à clé unique et une explication ou une explication non premable.\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "    retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Score chaque document\n",
    "    filtered_docs = []\n",
    "    search =\"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"Yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            search =\"Yes\"\n",
    "            continue\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs, \n",
    "            \"question\": question,\n",
    "            \"run_web_search\":search,\n",
    "            \n",
    "            }\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transforme la requête pour produire une meilleure question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): L'état actuel du graphique\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Met à jour la clé de question avec une question reformulée\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict= state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Ré-écriture de la question\n",
    "    re_write_prompt = PromptTemplate(\n",
    "        template=\"\"\"Vous êtes un rédacteur de questions qui convertit une question d'entrée en une meilleure version optimisée \\n\n",
    "            pour la récupération du vectorstore. Examinez l'entrée et essayez de raisonner sur l'intention sémantique sous-jacente et de formuler une question améliorée. \\n\n",
    "            Voici la question initiale: \\n\\n {question}. Question améliorée sans préambule: \\n \"\"\",\n",
    "    input_variables=[\"question\"]\n",
    "    )\n",
    "\n",
    "    llm=ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "    better_question_chain = re_write_prompt | llm | StrOutputParser()\n",
    "    better_question = better_question_chain.invoke({\"question\": question})\n",
    "    \n",
    "    return {\n",
    "        \"keys\":{\n",
    "            \"documents\": documents, \n",
    "            \"question\": better_question}}\n",
    "\n",
    "\n",
    "### Edges\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Recherche sur le Web basée sur la question reformulée en utilisantTavily API.\n",
    "\n",
    "    Args:\n",
    "        state (dict): L'état actuel du graphique\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Résultats Web annexés aux documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    state_dict= state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    web_search_tool=TavilySearchResults(k=5)\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\n",
    "        \"keys\":{\n",
    "            \"documents\": documents, \n",
    "            \"question\": question}\n",
    "    }\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Détermine s'il faut générer une réponse ou régénérer une question pour une recherche sur le Web.\n",
    "\n",
    "    Args:\n",
    "        state (dict): L'état actuel du graphique de l'agent, y compris toutes les clés.\n",
    "\n",
    "    Returns:\n",
    "        str: Next Node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict= state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    search= state_dict[\"run_web_search\"]\n",
    "\n",
    "    if search ==\"Yes\":\n",
    "        \n",
    "        print(\n",
    "            \"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        \n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Definition des noeuds\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"web_search\", web_search)  # web search\n",
    "\n",
    "# Construction du graphique\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search\")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compilation\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Noeud 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: GENERATE---\n",
      "\"Noeud 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Noeud 'generate':\"\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "inputs = {\n",
    "    \"keys\":{\n",
    "        \"question\": \"Comment fonctionne une cohabitation ?\"}\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Noeud\n",
    "        pprint(f\"Noeud '{key}':\")\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Réponse :  The document contains information about the French legislative elections of 2024 and the terms \"cohabitation\", \"majority absolute\", \"majority relative\", and \"motion of censure\".\n",
      "\n",
      "Cohabitation refers to a situation where a president and a parliament have different political colors, meaning they belong to different parties or ideologies. This was the case in 1986 when François Mitterrand, a president of the left, had to cohabit with a right-wing majority led by Jacques Chirac. In 1993, the same scenario occurred with Balladur taking office at Matignon after Chirac had already given his position. The term was also used in 1997 when the opposite situation happened, with a president of the right, Chirac, dissolving the parliament and finding himself with a left-wing majority. Emmanuel Macron may find himself in a similar situation in 2024.\n",
      "\n",
      "A motion of censure is a vote of no confidence in the government. It can be initiated by any member of the National Assembly and requires an absolute majority (more than half) of the members present to pass. If it succeeds, the government must resign.\n",
      "\n",
      "In the context of the 2024 legislative elections, Jordan Bardella may or may not have a majority after the second round of voting. The term \"majority\" can refer to an absolute majority (more than half) or a relative majority (the largest number of seats). If Bardella does not have a majority, there could be cohabitation between him and a parliament with different political colors.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Réponse :\",value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---DECIDE TO GENERATE---\n",
      "---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---TRANSFORM QUERY---\n",
      "\"Node 'transform_query':\"\n",
      "'\\n---\\n'\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "\n",
      " Réponse :  Le personnage principal dans le récent film \"Dune\" est Timothée Chalamet, qui joue le rôle de Leto Atréides.\n"
     ]
    }
   ],
   "source": [
    "#Test 2\n",
    "inputs = {\n",
    "    \"keys\":{\n",
    "        \"question\": \"quel est le nom du personnage principal dans le film sorti écemment Dune ?\"}\n",
    "}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "         pprint(f\"Node '{key}':\")\n",
    "        \n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Réponse :\",value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistraltest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
